{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# PulseDB / MIMIC single-subject (p043774) — time-aware protocol sweep\n",
        "# CLEANED:\n",
        "#   - Use PPG_F only (already per-segment min-max 0~1) => NO extra bandpass / NO extra input min-max\n",
        "#   - Keep 중요 로직(시간 split + block gap + time gap + block-test) 그대로\n",
        "# FIXED (critical):\n",
        "#   - Label scaling is TRAIN-ONLY per fold (NO future leakage)\n",
        "# ADDED:\n",
        "#   - Report MAE + SD (std of signed error) in mmHg for SBP/DBP\n",
        "# ============================================================\n",
        "\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import h5py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from scipy.interpolate import interp1d\n",
        "from scipy.integrate import trapezoid\n",
        "import pandas as pd\n",
        "\n",
        "# ==========================================\n",
        "# [0] 실험 환경 설정\n",
        "# ==========================================\n",
        "MAT_FILE = \"/content/drive/MyDrive/Colab Notebooks/PusleDB/p043774.mat\"\n",
        "SEGMENT_LIMIT = None          # None = all\n",
        "PAD_LEN = 200\n",
        "SEC_PER_SEGMENT = 10.0\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 100\n",
        "LR = 1e-3\n",
        "WEIGHT_DECAY = 1e-4\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "SEED = 42\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "print(f\"Using Device: {DEVICE}\")\n",
        "\n",
        "# ==========================================\n",
        "# [1] Re-sample + Prior feature\n",
        "# ==========================================\n",
        "def cubic_resample(ppg, target_len=PAD_LEN):\n",
        "    \"\"\"Cubic spline interpolation (fallback to linear)\"\"\"\n",
        "    x_old = np.linspace(0, 1, len(ppg))\n",
        "    x_new = np.linspace(0, 1, target_len)\n",
        "    if len(ppg) < 4:\n",
        "        return np.interp(x_new, x_old, ppg).astype(np.float32)\n",
        "    try:\n",
        "        f = interp1d(x_old, ppg, kind=\"cubic\", bounds_error=False, fill_value=\"extrapolate\")\n",
        "        return f(x_new).astype(np.float32)\n",
        "    except Exception:\n",
        "        return np.interp(x_new, x_old, ppg).astype(np.float32)\n",
        "\n",
        "def extract_multiscale_morph_features(ppg_01):\n",
        "    \"\"\"\n",
        "    Multi-scale Morphological Feature Extraction (44 dims)\n",
        "    - PPG_F already 0~1 per segment => use directly\n",
        "    \"\"\"\n",
        "    scales = [100, 150, 200, 250]\n",
        "    all_features = []\n",
        "\n",
        "    for scale in scales:\n",
        "        x = cubic_resample(ppg_01, scale)\n",
        "\n",
        "        peak_idx = int(np.argmax(x))\n",
        "        end_idx = scale - 1\n",
        "\n",
        "        vp = float(x[peak_idx])\n",
        "        vt = float(x[end_idx])\n",
        "        dv = vp - vt\n",
        "        vm = float(np.mean(x))\n",
        "        std_val = float(np.std(x))\n",
        "\n",
        "        tvp = peak_idx / scale\n",
        "\n",
        "        diff = np.diff(x)\n",
        "        kmax = float(np.max(diff)) if len(diff) > 0 else 0.0\n",
        "        tkmax = (int(np.argmax(diff)) / scale) if len(diff) > 0 else 0.0\n",
        "\n",
        "        amax = float(trapezoid(x[:peak_idx])) if peak_idx > 0 else 0.0\n",
        "\n",
        "        centered = x - vm\n",
        "        skew_approx = float(np.mean(centered**3) / (std_val**3)) if std_val > 0 else 0.0\n",
        "        kurt_approx = float(np.mean(centered**4) / (std_val**4)) if std_val > 0 else 0.0\n",
        "\n",
        "        all_features.extend([vp, vt, dv, vm, kmax, tkmax, amax, std_val, tvp, skew_approx, kurt_approx])\n",
        "\n",
        "    return np.array(all_features, dtype=np.float32)\n",
        "\n",
        "# ==========================================\n",
        "# [2] 데이터 로딩 (PPG_F only)\n",
        "# ==========================================\n",
        "def load_data_from_mat(mat_path, segment_limit=None):\n",
        "    segments, priors, targets = [], [], []\n",
        "    with h5py.File(mat_path, \"r\") as f:\n",
        "        refs = f[\"Subj_Wins\"][\"PPG_F\"][0]\n",
        "        sbps = f[\"Subj_Wins\"][\"SegSBP\"][0]\n",
        "        dbps = f[\"Subj_Wins\"][\"SegDBP\"][0]\n",
        "\n",
        "        total = min(len(refs), segment_limit) if segment_limit else len(refs)\n",
        "        for i in range(total):\n",
        "            ppg = f[refs[i]][()].squeeze().astype(np.float32)   # (1250,) already 0~1\n",
        "            sbp = float(f[sbps[i]][()][0][0])\n",
        "            dbp = float(f[dbps[i]][()][0][0])\n",
        "\n",
        "            segments.append(ppg)\n",
        "            priors.append(extract_multiscale_morph_features(ppg))\n",
        "            targets.append([sbp, dbp])\n",
        "\n",
        "            if i % 1000 == 0:\n",
        "                print(f\"  Processed {i}/{total} ...\")\n",
        "\n",
        "    return segments, np.stack(priors).astype(np.float32), np.array(targets, dtype=np.float32)\n",
        "\n",
        "# ==========================================\n",
        "# [3] Dataset (RAW y; label scaling is fold-wise train-only)\n",
        "# ==========================================\n",
        "class PPGDatasetRawY(Dataset):\n",
        "    def __init__(self, segments, priors, targets_mmHg):\n",
        "        self.segments = segments            # list of (1250,)\n",
        "        self.priors = priors                # (N, 44)\n",
        "        self.targets = targets_mmHg         # (N, 2) in mmHg\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.segments)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = cubic_resample(self.segments[idx], PAD_LEN)        # (200,)\n",
        "        x = torch.tensor(x, dtype=torch.float32).unsqueeze(0)  # (1, 200)\n",
        "\n",
        "        p = torch.tensor(self.priors[idx], dtype=torch.float32)\n",
        "        y = torch.tensor(self.targets[idx], dtype=torch.float32)  # RAW mmHg\n",
        "        return x, p, y\n",
        "\n",
        "# ==========================================\n",
        "# [4] Model\n",
        "# ==========================================\n",
        "class MorphCNNRegressor(nn.Module):\n",
        "    def __init__(self, prior_dim=44):\n",
        "        super().__init__()\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv1d(1, 32, 7, padding=3),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(2),\n",
        "\n",
        "            nn.Conv1d(32, 64, 5, padding=2),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(2),\n",
        "\n",
        "            nn.Conv1d(64, 128, 5, padding=2),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(2),\n",
        "\n",
        "            nn.Conv1d(128, 256, 3, padding=1),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool1d(1)\n",
        "        )\n",
        "\n",
        "        self.fc_prior = nn.Sequential(\n",
        "            nn.Linear(prior_dim, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.fc_out = nn.Sequential(\n",
        "            nn.Linear(256 + 256, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, prior):\n",
        "        feat = self.cnn(x).squeeze(-1)\n",
        "        pfeat = self.fc_prior(prior)\n",
        "        return self.fc_out(torch.cat([feat, pfeat], dim=1))\n",
        "\n",
        "# ==========================================\n",
        "# [5] Train-only label scaler (fold-wise)\n",
        "# ==========================================\n",
        "class LabelScaler2D:\n",
        "    \"\"\"Train-only scaling. Default: minmax.\"\"\"\n",
        "    def __init__(self, mode=\"minmax\", eps=1e-6):\n",
        "        assert mode in [\"minmax\", \"zscore\"]\n",
        "        self.mode = mode\n",
        "        self.eps = eps\n",
        "        self.fitted = False\n",
        "\n",
        "    def fit(self, y_train_mmHg: np.ndarray):\n",
        "        y = np.asarray(y_train_mmHg, dtype=np.float32)\n",
        "        if self.mode == \"minmax\":\n",
        "            self.y_min = y.min(axis=0)\n",
        "            self.y_max = y.max(axis=0)\n",
        "        else:\n",
        "            self.y_mean = y.mean(axis=0)\n",
        "            self.y_std = y.std(axis=0)\n",
        "        self.fitted = True\n",
        "        return self\n",
        "\n",
        "    def transform(self, y_mmHg: torch.Tensor) -> torch.Tensor:\n",
        "        assert self.fitted\n",
        "        if self.mode == \"minmax\":\n",
        "            y_min = torch.tensor(self.y_min, device=y_mmHg.device, dtype=y_mmHg.dtype)\n",
        "            y_max = torch.tensor(self.y_max, device=y_mmHg.device, dtype=y_mmHg.dtype)\n",
        "            return (y_mmHg - y_min) / (y_max - y_min + self.eps)\n",
        "        else:\n",
        "            y_mean = torch.tensor(self.y_mean, device=y_mmHg.device, dtype=y_mmHg.dtype)\n",
        "            y_std = torch.tensor(self.y_std, device=y_mmHg.device, dtype=y_mmHg.dtype)\n",
        "            return (y_mmHg - y_mean) / (y_std + self.eps)\n",
        "\n",
        "    def inverse(self, y_scaled: torch.Tensor) -> torch.Tensor:\n",
        "        assert self.fitted\n",
        "        if self.mode == \"minmax\":\n",
        "            y_min = torch.tensor(self.y_min, device=y_scaled.device, dtype=y_scaled.dtype)\n",
        "            y_max = torch.tensor(self.y_max, device=y_scaled.device, dtype=y_scaled.dtype)\n",
        "            return y_scaled * (y_max - y_min + self.eps) + y_min\n",
        "        else:\n",
        "            y_mean = torch.tensor(self.y_mean, device=y_scaled.device, dtype=y_scaled.dtype)\n",
        "            y_std = torch.tensor(self.y_std, device=y_scaled.device, dtype=y_scaled.dtype)\n",
        "            return y_scaled * (y_std + self.eps) + y_mean\n",
        "\n",
        "# ==========================================\n",
        "# [6] Train / Eval\n",
        "# ==========================================\n",
        "def set_seed(seed=SEED):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def train_one_model(train_loader, val_loader, scaler: LabelScaler2D):\n",
        "    model = MorphCNNRegressor(prior_dim=44).to(DEVICE)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    best_val = float(\"inf\")\n",
        "    best_state = None\n",
        "\n",
        "    for epoch in range(1, EPOCHS + 1):\n",
        "        model.train()\n",
        "        for x, p, y_mmHg in train_loader:\n",
        "            x, p, y_mmHg = x.to(DEVICE), p.to(DEVICE), y_mmHg.to(DEVICE)\n",
        "            y = scaler.transform(y_mmHg)\n",
        "            pred = model(x, p)\n",
        "            loss = criterion(pred, y)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "        model.eval()\n",
        "        val_losses = []\n",
        "        with torch.no_grad():\n",
        "            for x, p, y_mmHg in val_loader:\n",
        "                x, p, y_mmHg = x.to(DEVICE), p.to(DEVICE), y_mmHg.to(DEVICE)\n",
        "                y = scaler.transform(y_mmHg)\n",
        "                pred = model(x, p)\n",
        "                val_losses.append(float(criterion(pred, y).item()))\n",
        "        avg_val = float(np.mean(val_losses)) if len(val_losses) else float(\"inf\")\n",
        "\n",
        "        if avg_val < best_val:\n",
        "            best_val = avg_val\n",
        "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
        "\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "    return model\n",
        "\n",
        "def eval_mae_sd_mmHg(model, loader, scaler: LabelScaler2D):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      - MAE (mean absolute error) for SBP/DBP\n",
        "      - SD  (std of signed error) for SBP/DBP\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    errs = []\n",
        "    with torch.no_grad():\n",
        "        for x, p, y_mmHg in loader:\n",
        "            x, p, y_mmHg = x.to(DEVICE), p.to(DEVICE), y_mmHg.to(DEVICE)\n",
        "            pred_scaled = model(x, p)\n",
        "            pred_mmHg = scaler.inverse(pred_scaled)\n",
        "            err = (pred_mmHg - y_mmHg).detach().cpu().numpy()  # signed\n",
        "            errs.append(err)\n",
        "\n",
        "    if len(errs) == 0:\n",
        "        return dict(mae_sbp=np.nan, sd_sbp=np.nan, mae_dbp=np.nan, sd_dbp=np.nan, n=0)\n",
        "\n",
        "    E = np.concatenate(errs, axis=0)  # (N,2)\n",
        "    e_sbp, e_dbp = E[:, 0], E[:, 1]\n",
        "    return dict(\n",
        "        mae_sbp=float(np.mean(np.abs(e_sbp))),\n",
        "        sd_sbp=float(np.std(e_sbp, ddof=0)),\n",
        "        mae_dbp=float(np.mean(np.abs(e_dbp))),\n",
        "        sd_dbp=float(np.std(e_dbp, ddof=0)),\n",
        "        n=int(E.shape[0])\n",
        "    )\n",
        "\n",
        "# ==========================================\n",
        "# [7] Sweep Engine\n",
        "# ==========================================\n",
        "def segs_from_minutes(minutes: float) -> int:\n",
        "    return int((minutes * 60.0) / SEC_PER_SEGMENT)\n",
        "\n",
        "def run_timewise_holdout_gap_sweep():\n",
        "    set_seed(SEED)\n",
        "\n",
        "    segments, priors, targets_mmHg = load_data_from_mat(MAT_FILE, segment_limit=SEGMENT_LIMIT)\n",
        "    ds = PPGDatasetRawY(segments, priors, targets_mmHg)\n",
        "\n",
        "    total_len = len(ds)\n",
        "    print(f\"\\n[Data Ready] total_len={total_len}\")\n",
        "\n",
        "    sweep_train_dur = [3, 5, 10, 15]    # train length (min)\n",
        "    sweep_time_gap  = [0, 5, 10, 15]    # gap between train and test (min)\n",
        "\n",
        "    TEST_DURATION_MIN = 5\n",
        "    test_dur_segs = segs_from_minutes(TEST_DURATION_MIN)\n",
        "\n",
        "    # test = last 5 minutes\n",
        "    test_end = total_len\n",
        "    test_start = test_end - test_dur_segs\n",
        "    if test_start <= 0:\n",
        "        raise ValueError(\"Not enough data to allocate the test segment.\")\n",
        "\n",
        "    results = []\n",
        "    total_iter = len(sweep_train_dur) * len(sweep_time_gap)\n",
        "    it = 0\n",
        "    t0 = time.time()\n",
        "\n",
        "    for tr_dur in sweep_train_dur:\n",
        "        tr_dur_segs = segs_from_minutes(tr_dur)\n",
        "\n",
        "        for t_gap in sweep_time_gap:\n",
        "            gap_segs = segs_from_minutes(t_gap)\n",
        "\n",
        "            it += 1\n",
        "            elapsed = time.time() - t0\n",
        "            print(f\"[{it}/{total_iter}] Train:{tr_dur}m | TGap:{t_gap}m (elapsed {elapsed:.1f}s)\")\n",
        "\n",
        "            # train is before test_start, separated by gap\n",
        "            train_end = test_start - gap_segs\n",
        "            train_start = train_end - tr_dur_segs\n",
        "\n",
        "            # feasibility check\n",
        "            if train_start < 0 or train_end <= 0 or train_start >= train_end:\n",
        "                results.append({\n",
        "                    \"TrainDur_min\": tr_dur,\n",
        "                    \"TimeGap_min\": t_gap,\n",
        "                    \"MAE_SBP\": np.nan,\n",
        "                    \"SD_SBP\": np.nan,\n",
        "                    \"MAE_DBP\": np.nan,\n",
        "                    \"SD_DBP\": np.nan,\n",
        "                    \"train_n\": 0,\n",
        "                    \"val_n\": 0,\n",
        "                    \"test_n\": int(test_dur_segs),\n",
        "                    \"OK\": 0\n",
        "                })\n",
        "                continue\n",
        "\n",
        "            train_indices = list(range(train_start, train_end))\n",
        "            test_indices  = list(range(test_start, test_end))\n",
        "\n",
        "            # time-aware val split (last 20% of train)\n",
        "            n_total = len(train_indices)\n",
        "            n_val = max(1, int(n_total * 0.20))\n",
        "            if n_total - n_val < 1:\n",
        "                # too small train set\n",
        "                results.append({\n",
        "                    \"TrainDur_min\": tr_dur,\n",
        "                    \"TimeGap_min\": t_gap,\n",
        "                    \"MAE_SBP\": np.nan,\n",
        "                    \"SD_SBP\": np.nan,\n",
        "                    \"MAE_DBP\": np.nan,\n",
        "                    \"SD_DBP\": np.nan,\n",
        "                    \"train_n\": 0,\n",
        "                    \"val_n\": 0,\n",
        "                    \"test_n\": int(test_dur_segs),\n",
        "                    \"OK\": 0\n",
        "                })\n",
        "                continue\n",
        "\n",
        "            real_train_idx = train_indices[:-n_val]\n",
        "            val_idx        = train_indices[-n_val:]\n",
        "\n",
        "            # TRAIN-only scaler fit\n",
        "            y_train = targets_mmHg[np.array(real_train_idx)]\n",
        "            scaler = LabelScaler2D(mode=\"minmax\", eps=1e-6).fit(y_train)\n",
        "\n",
        "            train_loader = DataLoader(Subset(ds, real_train_idx), batch_size=BATCH_SIZE, shuffle=True)\n",
        "            val_loader   = DataLoader(Subset(ds, val_idx), batch_size=BATCH_SIZE, shuffle=False)\n",
        "            test_loader  = DataLoader(Subset(ds, test_indices), batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "            model = train_one_model(train_loader, val_loader, scaler)\n",
        "            stat = eval_mae_sd_mmHg(model, test_loader, scaler)\n",
        "\n",
        "            results.append({\n",
        "                \"TrainDur_min\": tr_dur,\n",
        "                \"TimeGap_min\": t_gap,\n",
        "                \"MAE_SBP\": stat[\"mae_sbp\"],\n",
        "                \"SD_SBP\":  stat[\"sd_sbp\"],\n",
        "                \"MAE_DBP\": stat[\"mae_dbp\"],\n",
        "                \"SD_DBP\":  stat[\"sd_dbp\"],\n",
        "                \"train_n\": len(real_train_idx),\n",
        "                \"val_n\":   len(val_idx),\n",
        "                \"test_n\":  len(test_indices),\n",
        "                \"OK\": 1\n",
        "            })\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "    print(\"\\n=== TIME-WISE HOLDOUT GAP SWEEP RESULTS ===\")\n",
        "    print(df[[\"TrainDur_min\",\"TimeGap_min\",\"MAE_SBP\",\"SD_SBP\",\"MAE_DBP\",\"SD_DBP\",\"train_n\",\"val_n\",\"test_n\",\"OK\"]])\n",
        "\n",
        "    out_csv = \"mimic_p043774_timewise_holdout_gap_sweep.csv\"\n",
        "    df.to_csv(out_csv, index=False)\n",
        "    print(f\"Saved: {out_csv}\")\n",
        "    return df\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    df = run_timewise_holdout_gap_sweep()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-FfzbZ4mvUa",
        "outputId": "2f2921c5-3540-4d3d-ecec-9dfc196b541b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Device: cuda\n",
            "  Processed 0/1943 ...\n",
            "  Processed 1000/1943 ...\n",
            "\n",
            "[Data Ready] total_len=1943\n",
            "[1/16] Train:3m | TGap:0m (elapsed 0.0s)\n",
            "[2/16] Train:3m | TGap:5m (elapsed 8.3s)\n",
            "[3/16] Train:3m | TGap:10m (elapsed 11.0s)\n",
            "[4/16] Train:3m | TGap:15m (elapsed 13.2s)\n",
            "[5/16] Train:5m | TGap:0m (elapsed 15.0s)\n",
            "[6/16] Train:5m | TGap:5m (elapsed 17.9s)\n",
            "[7/16] Train:5m | TGap:10m (elapsed 20.6s)\n",
            "[8/16] Train:5m | TGap:15m (elapsed 23.8s)\n",
            "[9/16] Train:10m | TGap:0m (elapsed 27.0s)\n",
            "[10/16] Train:10m | TGap:5m (elapsed 31.8s)\n",
            "[11/16] Train:10m | TGap:10m (elapsed 37.3s)\n",
            "[12/16] Train:10m | TGap:15m (elapsed 42.5s)\n",
            "[13/16] Train:15m | TGap:0m (elapsed 47.2s)\n",
            "[14/16] Train:15m | TGap:5m (elapsed 55.4s)\n",
            "[15/16] Train:15m | TGap:10m (elapsed 63.3s)\n",
            "[16/16] Train:15m | TGap:15m (elapsed 71.0s)\n",
            "\n",
            "=== TIME-WISE HOLDOUT GAP SWEEP RESULTS ===\n",
            "    TrainDur_min  TimeGap_min    MAE_SBP    SD_SBP   MAE_DBP    SD_DBP  \\\n",
            "0              3            0   5.362437  3.011940  2.590262  1.661627   \n",
            "1              3            5   3.018807  3.469489  3.635653  1.825620   \n",
            "2              3           10  11.042944  2.920604  7.630599  1.334565   \n",
            "3              3           15   8.642813  2.826842  5.593846  1.300027   \n",
            "4              5            0   5.358571  3.095910  1.584080  1.424123   \n",
            "5              5            5   6.733901  2.984851  1.270715  1.465382   \n",
            "6              5           10  10.784650  2.779893  7.580729  1.389199   \n",
            "7              5           15   7.171668  2.811239  5.724149  1.310403   \n",
            "8             10            0   4.918934  3.740980  1.631313  1.933724   \n",
            "9             10            5   5.602633  3.112478  1.255572  1.530768   \n",
            "10            10           10   9.296086  2.793251  6.839589  1.302620   \n",
            "11            10           15   7.303840  2.513029  5.538680  1.244630   \n",
            "12            15            0   5.297753  3.446317  1.496055  1.766795   \n",
            "13            15            5   2.463223  3.089459  1.835560  1.490705   \n",
            "14            15           10   8.134243  2.178605  6.024318  1.096667   \n",
            "15            15           15   5.695177  2.525624  4.801300  1.177460   \n",
            "\n",
            "    train_n  val_n  test_n  OK  \n",
            "0        15      3      30   1  \n",
            "1        15      3      30   1  \n",
            "2        15      3      30   1  \n",
            "3        15      3      30   1  \n",
            "4        24      6      30   1  \n",
            "5        24      6      30   1  \n",
            "6        24      6      30   1  \n",
            "7        24      6      30   1  \n",
            "8        48     12      30   1  \n",
            "9        48     12      30   1  \n",
            "10       48     12      30   1  \n",
            "11       48     12      30   1  \n",
            "12       72     18      30   1  \n",
            "13       72     18      30   1  \n",
            "14       72     18      30   1  \n",
            "15       72     18      30   1  \n",
            "Saved: mimic_p043774_timewise_holdout_gap_sweep.csv\n"
          ]
        }
      ]
    }
  ]
}