{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mPelJ4rNFszX",
        "outputId": "0018d937-ca39-4fa2-ac04-f495954d1095"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Device: cuda\n",
            "\n",
            "################################################################################\n",
            "[CROSS-SUBJECT LOSO] subjects=10 | filtering=False\n",
            "DIR: /content/drive/MyDrive/Colab Notebooks/PulseDB/Normal\n",
            "################################################################################\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "[LOAD] p004679.mat\n",
            "✅ (NoFilter) kept: 1862 / 1862\n",
            "  segments: 1862\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "[LOAD] p017795.mat\n",
            "✅ (NoFilter) kept: 1726 / 1726\n",
            "  segments: 1726\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "[LOAD] p027884.mat\n",
            "✅ (NoFilter) kept: 1802 / 1802\n",
            "  segments: 1802\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "[LOAD] p030670 (1).mat\n",
            "✅ (NoFilter) kept: 1764 / 1764\n",
            "  segments: 1764\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "[LOAD] p043774.mat\n",
            "✅ (NoFilter) kept: 1943 / 1943\n",
            "  segments: 1943\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "[LOAD] p059290.mat\n",
            "✅ (NoFilter) kept: 2053 / 2053\n",
            "  segments: 2053\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "[LOAD] p062917.mat\n",
            "✅ (NoFilter) kept: 1743 / 1743\n",
            "  segments: 1743\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "[LOAD] p081410.mat\n",
            "✅ (NoFilter) kept: 1741 / 1741\n",
            "  segments: 1741\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "[LOAD] p098382.mat\n",
            "✅ (NoFilter) kept: 1727 / 1727\n",
            "  segments: 1727\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "[LOAD] p099008.mat\n",
            "✅ (NoFilter) kept: 1883 / 1883\n",
            "  segments: 1883\n",
            "\n",
            "================================================================================\n",
            "[LOSO] Held-out TEST = p004679.mat (1/10)\n",
            "================================================================================\n",
            "    [ep 001/100] train=0.017874 | val=0.016784\n",
            "    [ep 025/100] train=0.003898 | val=0.016933\n",
            "    [ep 050/100] train=0.003543 | val=0.015119\n",
            "    [ep 075/100] train=0.003408 | val=0.012105\n",
            "    [ep 100/100] train=0.003411 | val=0.012830\n",
            "  Train/Val/Test sizes: 13106/3276/1862\n",
            "  SBP: MAE=13.4342 | SD=11.5684\n",
            "  DBP: MAE=9.8783 | SD=7.8236\n",
            "  elapsed: 1597.4s\n",
            "\n",
            "================================================================================\n",
            "[LOSO] Held-out TEST = p017795.mat (2/10)\n",
            "================================================================================\n",
            "    [ep 001/100] train=0.016635 | val=0.015503\n",
            "    [ep 025/100] train=0.004171 | val=0.015933\n",
            "    [ep 050/100] train=0.004008 | val=0.017819\n",
            "    [ep 075/100] train=0.003761 | val=0.020613\n",
            "    [ep 100/100] train=0.003670 | val=0.014851\n",
            "  Train/Val/Test sizes: 13215/3303/1726\n",
            "  SBP: MAE=12.1897 | SD=9.2384\n",
            "  DBP: MAE=13.4822 | SD=9.8662\n",
            "  elapsed: 1601.7s\n",
            "\n",
            "================================================================================\n",
            "[LOSO] Held-out TEST = p027884.mat (3/10)\n",
            "================================================================================\n",
            "    [ep 001/100] train=0.015950 | val=0.025978\n",
            "    [ep 025/100] train=0.004506 | val=0.014722\n",
            "    [ep 050/100] train=0.004234 | val=0.016698\n",
            "    [ep 075/100] train=0.004103 | val=0.019457\n",
            "    [ep 100/100] train=0.003894 | val=0.013905\n",
            "  Train/Val/Test sizes: 13154/3288/1802\n",
            "  SBP: MAE=8.0437 | SD=9.7667\n",
            "  DBP: MAE=3.9177 | SD=4.7698\n",
            "  elapsed: 1588.2s\n",
            "\n",
            "================================================================================\n",
            "[LOSO] Held-out TEST = p030670 (1).mat (4/10)\n",
            "================================================================================\n",
            "    [ep 001/100] train=0.016932 | val=0.017485\n",
            "    [ep 025/100] train=0.004294 | val=0.012001\n",
            "    [ep 050/100] train=0.003960 | val=0.013759\n",
            "    [ep 075/100] train=0.003690 | val=0.015789\n",
            "    [ep 100/100] train=0.003678 | val=0.009615\n",
            "  Train/Val/Test sizes: 13184/3296/1764\n",
            "  SBP: MAE=9.3659 | SD=7.3504\n",
            "  DBP: MAE=4.1980 | SD=4.7019\n",
            "  elapsed: 1573.4s\n",
            "\n",
            "================================================================================\n",
            "[LOSO] Held-out TEST = p043774.mat (5/10)\n",
            "================================================================================\n",
            "    [ep 001/100] train=0.017322 | val=0.013792\n",
            "    [ep 025/100] train=0.004408 | val=0.014363\n",
            "    [ep 050/100] train=0.004019 | val=0.015591\n",
            "    [ep 075/100] train=0.003842 | val=0.012383\n",
            "    [ep 100/100] train=0.003733 | val=0.014547\n",
            "  Train/Val/Test sizes: 13041/3260/1943\n",
            "  SBP: MAE=15.8071 | SD=12.7846\n",
            "  DBP: MAE=7.3228 | SD=6.3308\n",
            "  elapsed: 1547.4s\n",
            "\n",
            "================================================================================\n",
            "[LOSO] Held-out TEST = p059290.mat (6/10)\n",
            "================================================================================\n",
            "    [ep 001/100] train=0.020455 | val=0.020708\n",
            "    [ep 025/100] train=0.004969 | val=0.013198\n",
            "    [ep 050/100] train=0.004497 | val=0.013072\n",
            "    [ep 075/100] train=0.004297 | val=0.012326\n",
            "    [ep 100/100] train=0.004043 | val=0.015373\n",
            "  Train/Val/Test sizes: 12953/3238/2053\n",
            "  SBP: MAE=6.6842 | SD=9.1561\n",
            "  DBP: MAE=6.8997 | SD=7.1265\n",
            "  elapsed: 1541.9s\n",
            "\n",
            "================================================================================\n",
            "[LOSO] Held-out TEST = p062917.mat (7/10)\n",
            "================================================================================\n",
            "    [ep 001/100] train=0.013948 | val=0.017018\n",
            "    [ep 025/100] train=0.004032 | val=0.013159\n",
            "    [ep 050/100] train=0.003648 | val=0.012276\n",
            "    [ep 075/100] train=0.003607 | val=0.016045\n",
            "    [ep 100/100] train=0.003514 | val=0.010996\n",
            "  Train/Val/Test sizes: 13201/3300/1743\n",
            "  SBP: MAE=13.7209 | SD=13.9753\n",
            "  DBP: MAE=11.9842 | SD=8.2694\n",
            "  elapsed: 1582.7s\n",
            "\n",
            "================================================================================\n",
            "[LOSO] Held-out TEST = p081410.mat (8/10)\n",
            "================================================================================\n",
            "    [ep 001/100] train=0.015778 | val=0.013401\n",
            "    [ep 025/100] train=0.004176 | val=0.014741\n",
            "    [ep 050/100] train=0.003679 | val=0.017797\n",
            "    [ep 075/100] train=0.003645 | val=0.015565\n",
            "    [ep 100/100] train=0.003530 | val=0.014741\n",
            "  Train/Val/Test sizes: 13203/3300/1741\n",
            "  SBP: MAE=12.9786 | SD=6.9490\n",
            "  DBP: MAE=2.4862 | SD=3.6049\n",
            "  elapsed: 1586.4s\n",
            "\n",
            "================================================================================\n",
            "[LOSO] Held-out TEST = p098382.mat (9/10)\n",
            "================================================================================\n",
            "    [ep 001/100] train=0.018457 | val=0.019970\n",
            "    [ep 025/100] train=0.004362 | val=0.012335\n",
            "    [ep 050/100] train=0.003997 | val=0.011828\n",
            "    [ep 075/100] train=0.004056 | val=0.011326\n",
            "    [ep 100/100] train=0.003937 | val=0.011934\n",
            "  Train/Val/Test sizes: 13214/3303/1727\n",
            "  SBP: MAE=23.1370 | SD=9.5556\n",
            "  DBP: MAE=6.4351 | SD=6.5814\n",
            "  elapsed: 1563.8s\n",
            "\n",
            "================================================================================\n",
            "[LOSO] Held-out TEST = p099008.mat (10/10)\n",
            "================================================================================\n",
            "[SKIP FOLD] p099008.mat | reason: ValueError: Expected more than 1 value per channel when training, got input size torch.Size([1, 128])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "UnboundLocalError",
          "evalue": "cannot access local variable 'model' where it is not associated with a value",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3385680259.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;31m# ==========================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m     \u001b[0mrun_cross_subject_loso\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPULSEDB_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFILTERING\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_subjects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-3385680259.py\u001b[0m in \u001b[0;36mrun_cross_subject_loso\u001b[0;34m(pulsedb_dir, filtering, max_subjects)\u001b[0m\n\u001b[1;32m    518\u001b[0m             \u001b[0mskipped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m         \u001b[0;32mdel\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'model' where it is not associated with a value"
          ]
        }
      ],
      "source": [
        "import os, glob, time, random, gc\n",
        "import numpy as np\n",
        "import h5py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from scipy.interpolate import interp1d\n",
        "from scipy.integrate import trapezoid\n",
        "\n",
        "# ==========================================\n",
        "# [0] Experiment settings\n",
        "# ==========================================\n",
        "PULSEDB_DIR = \"/content/drive/MyDrive/Colab Notebooks/PulseDB/Normal\"\n",
        "SEGMENT_LIMIT = None\n",
        "PAD_LEN = 200\n",
        "SEC_PER_SEGMENT = 10.0\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 100\n",
        "LR = 1e-3\n",
        "WEIGHT_DECAY = 1e-4\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "SEED = 42\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "print(f\"Using Device: {DEVICE}\")\n",
        "\n",
        "# protocol config (kept, but LOSO will not use these)\n",
        "N_FOLDS = 5\n",
        "TIME_GAP_MIN = 5\n",
        "TEST_DUR_MIN = 5\n",
        "TRAIN_DUR_MIN = 15\n",
        "BLOCK_GAP_MIN = 3\n",
        "VAL_FRAC_IN_TRAIN = 0.20\n",
        "FILTERING = False\n",
        "\n",
        "# ==========================================\n",
        "# [1] Filtering + resample + priors\n",
        "# ==========================================\n",
        "def preprocess_ensemble_by_rpeaks(ppg_raw, rpeaks_raw, sbp, dbp, target_len=125, threshold_corr=0.7):\n",
        "    if not (50 <= sbp <= 250) or not (30 <= dbp <= 160):\n",
        "        return None\n",
        "\n",
        "    ppg = ppg_raw.squeeze()\n",
        "    rpeaks = rpeaks_raw.squeeze()\n",
        "    rpeaks = np.sort(rpeaks.astype(int))\n",
        "\n",
        "    beats = []\n",
        "    for i in range(len(rpeaks) - 1):\n",
        "        start, end = rpeaks[i], rpeaks[i + 1]\n",
        "        if start < 0 or end > len(ppg):\n",
        "            continue\n",
        "        beat_segment = ppg[start:end]\n",
        "        if len(beat_segment) < 20:\n",
        "            continue\n",
        "\n",
        "        x_old = np.linspace(0, 1, len(beat_segment))\n",
        "        x_new = np.linspace(0, 1, target_len)\n",
        "        f_interp = interp1d(x_old, beat_segment, kind='linear', fill_value=\"extrapolate\")\n",
        "        beats.append(f_interp(x_new))\n",
        "\n",
        "    if len(beats) < 5:\n",
        "        return None\n",
        "\n",
        "    beats = np.array(beats)\n",
        "    ensemble_avg = np.mean(beats, axis=0)\n",
        "\n",
        "    e_min, e_max = ensemble_avg.min(), ensemble_avg.max()\n",
        "    if e_max - e_min > 1e-6:\n",
        "        ensemble_avg = (ensemble_avg - e_min) / (e_max - e_min)\n",
        "\n",
        "    correlations = [np.corrcoef(ensemble_avg, b)[0, 1] for b in beats]\n",
        "    consistent_beats_count = sum(1 for c in correlations if c >= threshold_corr)\n",
        "    if (consistent_beats_count / len(beats)) < 0.7:\n",
        "        return None\n",
        "\n",
        "    return ensemble_avg.astype(np.float32)\n",
        "\n",
        "def cubic_resample(ppg, target_len=PAD_LEN):\n",
        "    x_old = np.linspace(0, 1, len(ppg))\n",
        "    x_new = np.linspace(0, 1, target_len)\n",
        "    if len(ppg) < 4:\n",
        "        return np.interp(x_new, x_old, ppg).astype(np.float32)\n",
        "    try:\n",
        "        f = interp1d(x_old, ppg, kind=\"cubic\", bounds_error=False, fill_value=\"extrapolate\")\n",
        "        return f(x_new).astype(np.float32)\n",
        "    except Exception:\n",
        "        return np.interp(x_new, x_old, ppg).astype(np.float32)\n",
        "\n",
        "def extract_multiscale_morph_features(ppg_01):\n",
        "    scales = [100, 150, 200, 250]\n",
        "    all_features = []\n",
        "    for scale in scales:\n",
        "        x = cubic_resample(ppg_01, scale)\n",
        "\n",
        "        peak_idx = int(np.argmax(x))\n",
        "        end_idx = scale - 1\n",
        "\n",
        "        vp = float(x[peak_idx])\n",
        "        vt = float(x[end_idx])\n",
        "        dv = vp - vt\n",
        "        vm = float(np.mean(x))\n",
        "        std_val = float(np.std(x))\n",
        "\n",
        "        tvp = peak_idx / scale\n",
        "\n",
        "        diff = np.diff(x)\n",
        "        kmax = float(np.max(diff)) if len(diff) > 0 else 0.0\n",
        "        tkmax = (int(np.argmax(diff)) / scale) if len(diff) > 0 else 0.0\n",
        "\n",
        "        amax = float(trapezoid(x[:peak_idx])) if peak_idx > 0 else 0.0\n",
        "\n",
        "        centered = x - vm\n",
        "        skew_approx = float(np.mean(centered**3) / (std_val**3)) if std_val > 0 else 0.0\n",
        "        kurt_approx = float(np.mean(centered**4) / (std_val**4)) if std_val > 0 else 0.0\n",
        "\n",
        "        all_features.extend([vp, vt, dv, vm, kmax, tkmax, amax, std_val, tvp, skew_approx, kurt_approx])\n",
        "\n",
        "    return np.array(all_features, dtype=np.float32)\n",
        "\n",
        "# ==========================================\n",
        "# [2] Load data\n",
        "# ==========================================\n",
        "def load_data_from_mat(mat_path, segment_limit=None, filtering=False):\n",
        "    segments, priors, targets = [], [], []\n",
        "    skip_bp, skip_noise = 0, 0\n",
        "\n",
        "    with h5py.File(mat_path, \"r\") as f:\n",
        "        sw = f[\"Subj_Wins\"]\n",
        "        ppg_refs = sw[\"PPG_F\"][0]\n",
        "        sbp_refs = sw[\"SegSBP\"][0]\n",
        "        dbp_refs = sw[\"SegDBP\"][0]\n",
        "\n",
        "        total = min(len(ppg_refs), segment_limit) if segment_limit else len(ppg_refs)\n",
        "\n",
        "        if not filtering:\n",
        "            for i in range(total):\n",
        "                ppg = f[ppg_refs[i]][()].squeeze().astype(np.float32)\n",
        "                sbp = float(f[sbp_refs[i]][()][0][0])\n",
        "                dbp = float(f[dbp_refs[i]][()][0][0])\n",
        "\n",
        "                segments.append(ppg)\n",
        "                priors.append(extract_multiscale_morph_features(ppg))\n",
        "                targets.append([sbp, dbp])\n",
        "\n",
        "            print(f\"✅ (NoFilter) kept: {len(segments)} / {total}\")\n",
        "            return segments, np.stack(priors).astype(np.float32), np.array(targets, dtype=np.float32)\n",
        "\n",
        "        # filtering=True\n",
        "        ecg_refs = sw[\"ECG_RPeaks\"][0]\n",
        "        for i in range(total):\n",
        "            ppg_raw = f[ppg_refs[i]][()]\n",
        "            sbp = float(f[sbp_refs[i]][()][0][0])\n",
        "            dbp = float(f[dbp_refs[i]][()][0][0])\n",
        "            rpeaks_raw = f[ecg_refs[i]][()]\n",
        "\n",
        "            processed_ppg = preprocess_ensemble_by_rpeaks(ppg_raw, rpeaks_raw, sbp, dbp)\n",
        "\n",
        "            if processed_ppg is None:\n",
        "                if not (50 <= sbp <= 250) or not (30 <= dbp <= 160):\n",
        "                    skip_bp += 1\n",
        "                else:\n",
        "                    skip_noise += 1\n",
        "                continue\n",
        "\n",
        "            segments.append(processed_ppg)\n",
        "            priors.append(extract_multiscale_morph_features(processed_ppg))\n",
        "            targets.append([sbp, dbp])\n",
        "\n",
        "    print(f\"✅ (Filter) kept: {len(segments)} / {total}\")\n",
        "    print(f\"❌ excluded: (bp_range={skip_bp}, noise/quality={skip_noise})\")\n",
        "    return segments, np.stack(priors).astype(np.float32), np.array(targets, dtype=np.float32)\n",
        "\n",
        "# ==========================================\n",
        "# [3] Dataset\n",
        "# ==========================================\n",
        "class PPGDatasetRawY(Dataset):\n",
        "    def __init__(self, segments, priors, targets_mmHg):\n",
        "        self.segments = segments\n",
        "        self.priors = priors\n",
        "        self.targets = targets_mmHg\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.segments)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = cubic_resample(self.segments[idx], PAD_LEN)\n",
        "        x = torch.tensor(x, dtype=torch.float32).unsqueeze(0)\n",
        "        p = torch.tensor(self.priors[idx], dtype=torch.float32)\n",
        "        y = torch.tensor(self.targets[idx], dtype=torch.float32)\n",
        "        return x, p, y\n",
        "\n",
        "# ==========================================\n",
        "# [4] Model\n",
        "# ==========================================\n",
        "class MorphCNNRegressor(nn.Module):\n",
        "    def __init__(self, prior_dim=44):\n",
        "        super().__init__()\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv1d(1, 32, 7, padding=3),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(2),\n",
        "\n",
        "            nn.Conv1d(32, 64, 5, padding=2),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(2),\n",
        "\n",
        "            nn.Conv1d(64, 128, 5, padding=2),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(2),\n",
        "\n",
        "            nn.Conv1d(128, 256, 3, padding=1),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool1d(1)\n",
        "        )\n",
        "\n",
        "        self.fc_prior = nn.Sequential(\n",
        "            nn.Linear(prior_dim, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.fc_out = nn.Sequential(\n",
        "            nn.Linear(256 + 256, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, prior):\n",
        "        feat = self.cnn(x).squeeze(-1)\n",
        "        pfeat = self.fc_prior(prior)\n",
        "        return self.fc_out(torch.cat([feat, pfeat], dim=1))\n",
        "\n",
        "# ==========================================\n",
        "# [5] Train-only label scaler\n",
        "# ==========================================\n",
        "class LabelScaler2D:\n",
        "    def __init__(self, mode=\"minmax\", eps=1e-6):\n",
        "        assert mode in [\"minmax\", \"zscore\"]\n",
        "        self.mode = mode\n",
        "        self.eps = eps\n",
        "        self.fitted = False\n",
        "\n",
        "    def fit(self, y_train_mmHg: np.ndarray):\n",
        "        y = np.asarray(y_train_mmHg, dtype=np.float32)\n",
        "        if self.mode == \"minmax\":\n",
        "            self.y_min = y.min(axis=0)\n",
        "            self.y_max = y.max(axis=0)\n",
        "        else:\n",
        "            self.y_mean = y.mean(axis=0)\n",
        "            self.y_std = y.std(axis=0)\n",
        "        self.fitted = True\n",
        "        return self\n",
        "\n",
        "    def transform(self, y_mmHg: torch.Tensor) -> torch.Tensor:\n",
        "        assert self.fitted\n",
        "        if self.mode == \"minmax\":\n",
        "            y_min = torch.tensor(self.y_min, device=y_mmHg.device, dtype=y_mmHg.dtype)\n",
        "            y_max = torch.tensor(self.y_max, device=y_mmHg.device, dtype=y_mmHg.dtype)\n",
        "            return (y_mmHg - y_min) / (y_max - y_min + self.eps)\n",
        "        else:\n",
        "            y_mean = torch.tensor(self.y_mean, device=y_mmHg.device, dtype=y_mmHg.dtype)\n",
        "            y_std = torch.tensor(self.y_std, device=y_mmHg.device, dtype=y_mmHg.dtype)\n",
        "            return (y_mmHg - y_mean) / (y_std + self.eps)\n",
        "\n",
        "    def inverse(self, y_scaled: torch.Tensor) -> torch.Tensor:\n",
        "        assert self.fitted\n",
        "        if self.mode == \"minmax\":\n",
        "            y_min = torch.tensor(self.y_min, device=y_scaled.device, dtype=y_scaled.dtype)\n",
        "            y_max = torch.tensor(self.y_max, device=y_scaled.device, dtype=y_scaled.dtype)\n",
        "            return y_scaled * (y_max - y_min + self.eps) + y_min\n",
        "        else:\n",
        "            y_mean = torch.tensor(self.y_mean, device=y_scaled.device, dtype=y_scaled.dtype)\n",
        "            y_std = torch.tensor(self.y_std, device=y_scaled.device, dtype=y_scaled.dtype)\n",
        "            return y_scaled * (y_std + self.eps) + y_mean\n",
        "\n",
        "# ==========================================\n",
        "# [6] Train / Eval\n",
        "# ==========================================\n",
        "def set_seed(seed=SEED):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def train_one_model(train_loader, val_loader, scaler: LabelScaler2D):\n",
        "    model = MorphCNNRegressor(prior_dim=44).to(DEVICE)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    best_val = float(\"inf\")\n",
        "    best_state = None\n",
        "\n",
        "    for epoch in range(1, EPOCHS + 1):\n",
        "        # -------------------\n",
        "        # Train\n",
        "        # -------------------\n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        for x, p, y_mmHg in train_loader:\n",
        "            x, p, y_mmHg = x.to(DEVICE), p.to(DEVICE), y_mmHg.to(DEVICE)\n",
        "            y = scaler.transform(y_mmHg)\n",
        "\n",
        "            pred = model(x, p)\n",
        "            loss = criterion(pred, y)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            train_losses.append(float(loss.item()))\n",
        "\n",
        "        avg_train = float(np.mean(train_losses)) if len(train_losses) else float(\"inf\")\n",
        "\n",
        "        # -------------------\n",
        "        # Val\n",
        "        # -------------------\n",
        "        model.eval()\n",
        "        val_losses = []\n",
        "        with torch.no_grad():\n",
        "            for x, p, y_mmHg in val_loader:\n",
        "                x, p, y_mmHg = x.to(DEVICE), p.to(DEVICE), y_mmHg.to(DEVICE)\n",
        "                y = scaler.transform(y_mmHg)\n",
        "\n",
        "                pred = model(x, p)\n",
        "                val_losses.append(float(criterion(pred, y).item()))\n",
        "        avg_val = float(np.mean(val_losses)) if len(val_losses) else float(\"inf\")\n",
        "\n",
        "        # ✅ 25 epoch마다 로그 출력 (+ 첫 epoch, 마지막 epoch도 출력)\n",
        "        if (epoch == 1) or (epoch % 25 == 0) or (epoch == EPOCHS):\n",
        "            print(f\"    [ep {epoch:03d}/{EPOCHS}] train={avg_train:.6f} | val={avg_val:.6f}\")\n",
        "\n",
        "        # Best checkpoint by val loss\n",
        "        if avg_val < best_val:\n",
        "            best_val = avg_val\n",
        "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
        "\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "    return model\n",
        "\n",
        "\n",
        "def eval_mae_sd_mmHg(model, loader, scaler: LabelScaler2D):\n",
        "    model.eval()\n",
        "    errs = []\n",
        "    with torch.no_grad():\n",
        "        for x, p, y_mmHg in loader:\n",
        "            x, p, y_mmHg = x.to(DEVICE), p.to(DEVICE), y_mmHg.to(DEVICE)\n",
        "            pred_scaled = model(x, p)\n",
        "            pred_mmHg = scaler.inverse(pred_scaled)\n",
        "            err = (pred_mmHg - y_mmHg).detach().cpu().numpy()\n",
        "            errs.append(err)\n",
        "\n",
        "    if len(errs) == 0:\n",
        "        return dict(mae_sbp=np.nan, sd_sbp=np.nan, mae_dbp=np.nan, sd_dbp=np.nan, n=0)\n",
        "\n",
        "    E = np.concatenate(errs, axis=0)\n",
        "    e_sbp, e_dbp = E[:, 0], E[:, 1]\n",
        "    return dict(\n",
        "        mae_sbp=float(np.mean(np.abs(e_sbp))),\n",
        "        sd_sbp=float(np.std(e_sbp, ddof=0)),\n",
        "        mae_dbp=float(np.mean(np.abs(e_dbp))),\n",
        "        sd_dbp=float(np.std(e_dbp, ddof=0)),\n",
        "        n=int(E.shape[0])\n",
        "    )\n",
        "\n",
        "# ==========================================\n",
        "# [7] LOSO helpers\n",
        "# ==========================================\n",
        "def build_patient_dataset(mat_path: str, filtering: bool):\n",
        "    segments, priors, targets_mmHg = load_data_from_mat(mat_path, segment_limit=SEGMENT_LIMIT, filtering=filtering)\n",
        "    ds = PPGDatasetRawY(segments, priors, targets_mmHg)\n",
        "    return ds, targets_mmHg\n",
        "\n",
        "def concat_datasets(datasets):\n",
        "    # torch.utils.data.ConcatDataset exists, but we also need y targets for scaler fit\n",
        "    from torch.utils.data import ConcatDataset\n",
        "    return ConcatDataset(datasets)\n",
        "\n",
        "def collect_targets_from_concat(targets_list):\n",
        "    # targets_list: list of np arrays (Ni,2)\n",
        "    return np.concatenate(targets_list, axis=0).astype(np.float32)\n",
        "\n",
        "def summarize_loso_fold_stats(valid_stats):\n",
        "    # valid_stats is list of dicts per held-out patient\n",
        "    mae_sbp = np.array([v[\"mae_sbp\"] for v in valid_stats], dtype=np.float32)\n",
        "    mae_dbp = np.array([v[\"mae_dbp\"] for v in valid_stats], dtype=np.float32)\n",
        "    sd_sbp  = np.array([v[\"sd_sbp\"]  for v in valid_stats], dtype=np.float32)\n",
        "    sd_dbp  = np.array([v[\"sd_dbp\"]  for v in valid_stats], dtype=np.float32)\n",
        "\n",
        "    return {\n",
        "        \"avg_mae_sbp\": float(mae_sbp.mean()),\n",
        "        \"avg_mae_dbp\": float(mae_dbp.mean()),\n",
        "        \"avg_sd_sbp\":  float(sd_sbp.mean()),\n",
        "        \"avg_sd_dbp\":  float(sd_dbp.mean()),\n",
        "        \"worst_mae_sbp\": float(mae_sbp.max()),\n",
        "        \"worst_mae_dbp\": float(mae_dbp.max()),\n",
        "        \"std_mae_sbp\": float(mae_sbp.std(ddof=0)),\n",
        "        \"std_mae_dbp\": float(mae_dbp.std(ddof=0)),\n",
        "        \"valid_folds\": int(len(valid_stats)),\n",
        "    }\n",
        "\n",
        "# ==========================================\n",
        "# [8] Cross-subject LOSO (10 patients)\n",
        "# ==========================================\n",
        "def run_cross_subject_loso(pulsedb_dir: str, filtering: bool, max_subjects=10):\n",
        "    set_seed(SEED)\n",
        "\n",
        "    patient_files = sorted(glob.glob(os.path.join(pulsedb_dir, \"p*.mat\")))\n",
        "    if len(patient_files) == 0:\n",
        "        raise FileNotFoundError(f\"No .mat files found under: {pulsedb_dir}\")\n",
        "\n",
        "    # use first 10 (or less if fewer exist)\n",
        "    patient_files = patient_files[:max_subjects]\n",
        "    print(\"\\n\" + \"#\" * 80)\n",
        "    print(f\"[CROSS-SUBJECT LOSO] subjects={len(patient_files)} | filtering={filtering}\")\n",
        "    print(f\"DIR: {pulsedb_dir}\")\n",
        "    print(\"#\" * 80)\n",
        "\n",
        "    # pre-load each patient's dataset once (so we don't repeatedly read .mat)\n",
        "    per_ds = []\n",
        "    per_y  = []\n",
        "    per_n  = []\n",
        "    for fp in patient_files:\n",
        "        print(\"\\n\" + \"-\" * 80)\n",
        "        print(f\"[LOAD] {os.path.basename(fp)}\")\n",
        "        ds_i, y_i = build_patient_dataset(fp, filtering=filtering)\n",
        "        per_ds.append(ds_i)\n",
        "        per_y.append(y_i)\n",
        "        per_n.append(len(ds_i))\n",
        "        print(f\"  segments: {len(ds_i)}\")\n",
        "\n",
        "    fold_stats = []\n",
        "    skipped = []\n",
        "\n",
        "    t0_all = time.time()\n",
        "\n",
        "    for test_idx, test_fp in enumerate(patient_files):\n",
        "        test_name = os.path.basename(test_fp)\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(f\"[LOSO] Held-out TEST = {test_name} ({test_idx+1}/{len(patient_files)})\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        # train = all except test\n",
        "        train_ds_list = [per_ds[i] for i in range(len(patient_files)) if i != test_idx]\n",
        "        train_y_list  = [per_y[i]  for i in range(len(patient_files)) if i != test_idx]\n",
        "        test_ds       = per_ds[test_idx]\n",
        "\n",
        "        if len(test_ds) < 1:\n",
        "            skipped.append((test_name, \"empty test subject\"))\n",
        "            continue\n",
        "\n",
        "        train_concat = concat_datasets(train_ds_list)\n",
        "        y_train_all  = collect_targets_from_concat(train_y_list)\n",
        "\n",
        "        n_total = len(train_concat)\n",
        "        n_val = max(1, int(n_total * VAL_FRAC_IN_TRAIN))\n",
        "        if n_total - n_val < 1:\n",
        "            skipped.append((test_name, \"train too small after val split\"))\n",
        "            continue\n",
        "\n",
        "        # IMPORTANT: we split train/val by index on concatenated dataset (no shuffling here)\n",
        "        train_indices = list(range(0, n_total - n_val))\n",
        "        val_indices   = list(range(n_total - n_val, n_total))\n",
        "        test_indices  = list(range(0, len(test_ds)))\n",
        "\n",
        "        # scaler fit on TRAIN ONLY (exclude val)\n",
        "        y_train_only = y_train_all[np.array(train_indices)]\n",
        "        scaler = LabelScaler2D(mode=\"minmax\", eps=1e-6).fit(y_train_only)\n",
        "\n",
        "        train_loader = DataLoader(Subset(train_concat, train_indices), batch_size=BATCH_SIZE, shuffle=True)\n",
        "        val_loader   = DataLoader(Subset(train_concat, val_indices),   batch_size=BATCH_SIZE, shuffle=False)\n",
        "        test_loader  = DataLoader(Subset(test_ds, test_indices),       batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "        t0 = time.time()\n",
        "        try:\n",
        "            model = train_one_model(train_loader, val_loader, scaler)\n",
        "            stat = eval_mae_sd_mmHg(model, test_loader, scaler)\n",
        "            elapsed = time.time() - t0\n",
        "\n",
        "            stat.update({\n",
        "                \"test_subject\": test_name,\n",
        "                \"train_subjects\": len(patient_files) - 1,\n",
        "                \"train_n\": len(train_indices),\n",
        "                \"val_n\": len(val_indices),\n",
        "                \"test_n\": len(test_indices),\n",
        "                \"elapsed_s\": float(elapsed),\n",
        "            })\n",
        "            fold_stats.append(stat)\n",
        "\n",
        "            print(f\"  Train/Val/Test sizes: {len(train_indices)}/{len(val_indices)}/{len(test_indices)}\")\n",
        "            print(f\"  SBP: MAE={stat['mae_sbp']:.4f} | SD={stat['sd_sbp']:.4f}\")\n",
        "            print(f\"  DBP: MAE={stat['mae_dbp']:.4f} | SD={stat['sd_dbp']:.4f}\")\n",
        "            print(f\"  elapsed: {elapsed:.1f}s\")\n",
        "\n",
        "        except Exception as e:\n",
        "            reason = f\"{type(e).__name__}: {e}\"\n",
        "            print(f\"[SKIP FOLD] {test_name} | reason: {reason}\")\n",
        "            skipped.append((test_name, reason))\n",
        "\n",
        "        del model\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    valid = [fs for fs in fold_stats if fs is not None and np.isfinite(fs[\"mae_sbp\"])]\n",
        "    print(\"\\n\" + \"#\" * 80)\n",
        "    print(f\"[LOSO SUMMARY] valid={len(valid)}/{len(patient_files)} | filtering={filtering}\")\n",
        "    print(\"#\" * 80)\n",
        "\n",
        "    if len(valid) == 0:\n",
        "        print(\"No valid folds.\")\n",
        "        return fold_stats, skipped\n",
        "\n",
        "    summ = summarize_loso_fold_stats(valid)\n",
        "    print(f\"ValidFolds: {summ['valid_folds']}/{len(patient_files)}\")\n",
        "    print(f\"Avg   SBP: MAE={summ['avg_mae_sbp']:.4f} | SD={summ['avg_sd_sbp']:.4f}\")\n",
        "    print(f\"Avg   DBP: MAE={summ['avg_mae_dbp']:.4f} | SD={summ['avg_sd_dbp']:.4f}\")\n",
        "    print(f\"Worst SBP: MAE={summ['worst_mae_sbp']:.4f} | StdAcrossFolds(MAE)={summ['std_mae_sbp']:.4f}\")\n",
        "    print(f\"Worst DBP: MAE={summ['worst_mae_dbp']:.4f} | StdAcrossFolds(MAE)={summ['std_mae_dbp']:.4f}\")\n",
        "\n",
        "    if skipped:\n",
        "        print(\"\\n\" + \"#\" * 80)\n",
        "        print(\"[SKIPPED FOLDS]\")\n",
        "        print(\"#\" * 80)\n",
        "        for p, r in skipped:\n",
        "            print(f\"- {p}: {r}\")\n",
        "\n",
        "    print(f\"\\n[ALL DONE] Total elapsed: {time.time() - t0_all:.1f}s\")\n",
        "    return fold_stats, skipped\n",
        "\n",
        "# ==========================================\n",
        "# [MAIN]\n",
        "# ==========================================\n",
        "if __name__ == \"__main__\":\n",
        "    run_cross_subject_loso(PULSEDB_DIR, filtering=FILTERING, max_subjects=10)\n"
      ]
    }
  ]
}